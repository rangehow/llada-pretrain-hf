2025-08-26 12:10:38,960	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
Arguments: Namespace(model_name_or_path='/mnt/dolphinfs/ssd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/diffusion/model_output/llada_1B_100b_finefineweb/checkpoint-257000', model_type='masked', tasks='arc_easy', batch_size=32, limit=0, output_dir='evaluation_results', num_workers=4, trust_remote_code=True)
Results will be saved to: evaluation_results/llada_1B_100b_finefineweb
Model short name: llada_1B_100b_finefineweb
Found 2 GPUs. Creating one actor per GPU.
Warning: Batch size 32 might be very slow for masked LMs. Consider reducing it.

----- Starting evaluation for task: arc_easy -----
Processing task: arc_easy (Logprob Mode)
[arc_easy] Expanding for logprob evaluation:   0%|          | 0/570 [00:00<?, ? examples/s][arc_easy] Expanding for logprob evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 2767.37 examples/s][arc_easy] Expanding for logprob evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 2635.39 examples/s]
Starting Ray evaluation on 2281 samples with 2 GPUs...
Distributing batches to GPUs:   0%|          | 0/72 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Distributing batches to GPUs:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/72 [00:00<00:00, 296.43it/s]Distributing batches to GPUs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:00<00:00, 389.22it/s]
Collecting results from GPUs:   0%|          | 0/72 [00:00<?, ?it/s][36m(Evaluator pid=131022)[0m You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Collecting results from GPUs:   1%|â–         | 1/72 [00:14<17:33, 14.83s/it]Collecting results from GPUs:   7%|â–‹         | 5/72 [00:14<02:30,  2.24s/it]Collecting results from GPUs:  12%|â–ˆâ–Ž        | 9/72 [00:15<01:04,  1.03s/it]Collecting results from GPUs:  18%|â–ˆâ–Š        | 13/72 [00:15<00:34,  1.70it/s]Collecting results from GPUs:  24%|â–ˆâ–ˆâ–Ž       | 17/72 [00:15<00:20,  2.68it/s]Collecting results from GPUs:  29%|â–ˆâ–ˆâ–‰       | 21/72 [00:15<00:12,  3.99it/s]Collecting results from GPUs:  35%|â–ˆâ–ˆâ–ˆâ–      | 25/72 [00:15<00:08,  5.68it/s]Collecting results from GPUs:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 29/72 [00:15<00:05,  7.68it/s]Collecting results from GPUs:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 33/72 [00:15<00:03, 10.22it/s]Collecting results from GPUs:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 38/72 [00:15<00:02, 13.89it/s]Collecting results from GPUs:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 42/72 [00:16<00:01, 16.64it/s]Collecting results from GPUs:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 46/72 [00:16<00:01, 18.51it/s]Collecting results from GPUs:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 50/72 [00:16<00:01, 21.23it/s]Collecting results from GPUs:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 54/72 [00:16<00:00, 24.11it/s]Collecting results from GPUs:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 58/72 [00:16<00:00, 25.83it/s]Collecting results from GPUs:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 63/72 [00:16<00:00, 28.33it/s]Collecting results from GPUs:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 67/72 [00:16<00:00, 30.78it/s]Collecting results from GPUs:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 71/72 [00:16<00:00, 31.03it/s]Collecting results from GPUs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:16<00:00,  4.24it/s]
[36m(Evaluator pid=131024)[0m You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

=== ARC_EASY Evaluation Results ===
Model: llada_1B_100b_finefineweb
Accuracy: 0.2596 (25.96%) (148/570)
âœ“ Detailed results saved to: evaluation_results/llada_1B_100b_finefineweb/arc_easy.json
âœ“ Summary appended to: evaluation_results/llada_1B_100b_finefineweb/summary.jsonl

âœ“ Overall summary saved to: evaluation_results/llada_1B_100b_finefineweb/overall_summary.json
âœ“ Average accuracy across all tasks: 0.2596
