2025-08-25 11:23:44,787	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
Arguments: Namespace(model_name_or_path='/mnt/dolphinfs/ssd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/diffusion/model_output_old/llada_400m_50B_token/checkpoint-113209', model_type='masked', tasks='arc_easy', batch_size=32, limit=0, output_dir='evaluation_results', num_workers=4, trust_remote_code=True)
Found 2 GPUs. Creating one actor per GPU.
Warning: Batch size 32 might be very slow for masked LMs. Consider reducing it.

----- Starting evaluation for task: arc_easy -----
Processing task: arc_easy (Logprob Mode)
[arc_easy] Expanding for logprob evaluation:   0%|          | 0/570 [00:00<?, ? examples/s][arc_easy] Expanding for logprob evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 2690.13 examples/s][arc_easy] Expanding for logprob evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 2557.35 examples/s]
Starting Ray evaluation on 2281 samples with 2 GPUs...
Distributing batches to GPUs:   0%|          | 0/72 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Distributing batches to GPUs:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 35/72 [00:00<00:00, 346.15it/s]Distributing batches to GPUs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:00<00:00, 421.56it/s]
Collecting results from GPUs:   0%|          | 0/72 [00:00<?, ?it/s][36m(Evaluator pid=141796)[0m You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Collecting results from GPUs:   1%|â–         | 1/72 [00:13<15:32, 13.13s/it]Collecting results from GPUs:  10%|â–‰         | 7/72 [00:13<01:30,  1.39s/it]Collecting results from GPUs:  18%|â–ˆâ–Š        | 13/72 [00:13<00:36,  1.62it/s]Collecting results from GPUs:  28%|â–ˆâ–ˆâ–Š       | 20/72 [00:13<00:17,  3.06it/s]Collecting results from GPUs:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 28/72 [00:13<00:08,  5.27it/s]Collecting results from GPUs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 36/72 [00:13<00:04,  8.19it/s]Collecting results from GPUs:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 44/72 [00:13<00:02, 11.86it/s]Collecting results from GPUs:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 52/72 [00:13<00:01, 16.37it/s]Collecting results from GPUs:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 60/72 [00:14<00:00, 21.73it/s]Collecting results from GPUs:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 68/72 [00:14<00:00, 27.96it/s]Collecting results from GPUs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:14<00:00,  5.06it/s]
[36m(Evaluator pid=141824)[0m You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

=== ARC_EASY Evaluation Results ===
Model: /mnt/dolphinfs/ssd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/diffusion/model_output_old/llada_400m_50B_token/checkpoint-113209
Accuracy: 0.2368 (23.68%) (135/570)
Results for task arc_easy saved to evaluation_results/_mnt_dolphinfs_ssd_pool_docker_user_hadoop-aipnlp_ruanjunhao04_diffusion_model_output_old_llada_400m_50B_token_checkpoint-113209__arc_easy.json
